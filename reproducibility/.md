# Reproduce (Test it yourself)

## Setup project

Create new project in [Google Cloud Console](https://console.cloud.google.com/) &rarr; switch to that newly created project


## Setup VM
Create instance with the following specifications:
- region: closest to you with low co2
- Machine Type: e2-standard-4 (4 vCPU, 16 GB Memory)
- Change boot disk: Ubuntu 20.04 LTS, Size: 30 GB
- you might have to enable the compute engine API if you haven't created a VM on this account before

## Setup SSH to VM

- in your local terminal: `ssh-keygen -t rsa -f ~/.ssh/<DESIREDNAMEOFYOURKEY> -C <DESIREDUSERNAMEONVM> -b 2048` <br>
- cat out the public key: `cat .ssh/capstone.pub` <br>
- copy output, go to VM instance on Google Cloud Console & paste ssh key under Metadata
- Go back to the VM and start it, copy the external IP
- Create a config-file locally under your .ssh directory with the following content:
```
Host <hostname to use when connecting>
HostName <external IP>
User <DESIREDUSERNAMEONVM you specified in ssh-keygen command>
IdentityFile <path to your private key> e.g.  ~/.ssh/privatekey
```
- go to your local terminal and type `ssh <hostname to use when connecting>`
  <br>
    &rarr; you are now connected to your VM

## Connecting and setting up VSCode

- install VS Code locally if you don't have it already 
- search Extensions for SSH and install Remote-SSH from Microsoft
- install Python extension if you need to
- in the lower left hand corner click the green icon to Open a Remote Window
- choose "Connect to Host..." and either choose your ssh connection or type in the name your chose for Host in your config

## Create Service Account
    
- go to IAM & Admin &rarr; Service Accounts on Google Cloud Console
- create service account
- grant the following roles:
    - Viewer
    - Storage Admin 
    - Storage Object Admin 
    - BigQuery Admin
- click continue and navigate to the three dots on the right side and click on manage keys
- choose: Add Key & &rarr; Create new key &rarr; JSON &rarr; Create 
- key will be downloaded onto local computer 
    
## Setup your VM

1. Open terminal in VS Code and run:
   <br>
   `wget https://repo.anaconda.com/archive/Anaconda3-2023.03-Linux-x86_64.sh`
2. make download executable and run it:
   <br>
   `chmod +x Anaconda3-2023.03-Linux-x86_64.sh`
   <br>
   `./Anaconda3-2023.03-Linux-x86_64.sh`
   <br> type 'yes' when prompted
3. run 
   <br>
   `sudo apt-get update`
4. clone repo
   <br>
   `git clone https://github.com/PandaKata/dezoomcamp-project.git`
5. make new directory for credentials
   <br>
   `mkdir -p .google/credentials`
   <br>
   move the credentials file we downloaded before in there
6. add the following to the .bashrc file:
   <br>
   `export GOOGLE_APPLICATION_CREDENTIALS=~/.google/credentials/<name-of-creds-file>.json`
   <br>
   then run
   <br>
   `source .bashrc`
   <br>
   and authenticate by running
   <br>
   `gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS`
7. setup terraform
   <br>
   run 
   <br>
```
curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add -
sudo apt-add-repository "deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main"
sudo apt-get update && sudo apt-get install terraform
```
- `cd` to the terraform directory
- change the variables.tf file with your corresponding variables, I would recommend to leave the name of the dataset, table and bucket as they are; otherwise you need to change them in the mage flows and dbt:

- run
  <br>
```
terraform init
terraform apply
```
- type 'yes' when prompted
  
1. create virtual environment
   <br>
   run
   <br>
```
conda create -n capstone python=3.10
conda activate capstone
pip install -r requirements.txt
```
    
    
## Setup Mage


  
## Setup dbt

1. create a [dbt cloud account](https://www.getdbt.com/signup/) 
2. create a new project and connect it to your [warehouse](https://docs.getdbt.com/docs/cloud/manage-access/set-up-bigquery-oauth); more detailed instruction can be found [here](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/dbt_cloud_setup.md).
3. fork [my repo](https://github.com/BrysonShitsukane77/stock-market-analysis-dashboard) if you haven't done it yet.
4. setup a repo in dbt cloud:
  - Choose git clone and paste the ssh link from your github fork
  
  - copy key
  
  - in the forked repo go to Settings and then Deploy Keys &rarr; paste the key; enable write access
  
  - go to your project settings in dbt cloud and change the subdirectory to 'dbt'
  
5. go "Develop" on dbt cloud &rarr; there you should see the repo; navigate to the dbt directory
6. run `dbt deps` in the console, so your environment is ready
7.  navigate to models &rarr; staging &rarr; schema.yml and replace variables with your own where necessary:
  
8.  you may also need to go into the .sql files and update the names to match what is in Big Query
9.  repeat for core directory

## Run dbt in production
1. go to environments in dbt cloud and create environment 
  
2. go to jobs in dbt cloud and create new job with the following parameters; this will schedule the dbt transformations daily.
  
 

## Visualization in Looker Studio
go to [Looker Studio](https://lookerstudio.google.com/) &rarr; create &rarr; BigQuery &rarr; choose your project, dataset & transformed table

## Flows
Your flows / jobs should look like this, when everything is running correctly:
